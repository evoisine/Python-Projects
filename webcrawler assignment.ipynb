{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webcrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Main Page of english Wikipedia as starting URL\n",
    "url = \"https://en.wikipedia.org/wiki/Black_Country,_New_Road\"\n",
    "\n",
    "# initialize dict to store\n",
    "url_info = {'source_url': [], 'link_url': [], 'link_title': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(url) as response:\n",
    "    parsed_page = BeautifulSoup(response.read())\n",
    "\n",
    "while len(url_info['link_url']) < 100:\n",
    "\n",
    "    # get the href attribute for each A tag\n",
    "    url_links = list(map(lambda x: x.attrs['href'] if ('href' in x.attrs) else None, parsed_page.find_all('a')))\n",
    "\n",
    "    # remove None values passed by <a> tags that had no href attribute\n",
    "    url_links = list(filter(None, url_links))\n",
    "\n",
    "    # convert relatives URLs to absolute URLs using the page URL they appear on\n",
    "    url_links = list(map(lambda x: urllib.parse.urljoin(url, x), url_links))\n",
    "\n",
    "    num_scraped = 0\n",
    "    source = urllib.parse.urlparse(url)\n",
    "\n",
    "    # loop over list of absolute URLs\n",
    "    for link in url_links:\n",
    "\n",
    "        parsed_link = urllib.parse.urlparse(link)\n",
    "\n",
    "        # check if link is in same domain as source, skip if not\n",
    "        if parsed_link.netloc != source.netloc:\n",
    "            continue\n",
    "\n",
    "        # check if link has same path as source (same page), skip if it does\n",
    "        if parsed_link.path == source.path:\n",
    "            continue\n",
    "\n",
    "        # check if link has been collected already, skip if it has\n",
    "        if link in url_info['link_url']:\n",
    "            continue\n",
    "\n",
    "        # retrieve title of url\n",
    "        link_title = BeautifulSoup(urllib.request.urlopen(link)).title.get_text()\n",
    "\n",
    "        url_info['source_url'].append(url)\n",
    "        url_info['link_url'].append(link)\n",
    "        url_info['link_title'].append(link_title)\n",
    "\n",
    "        num_scraped += 1\n",
    "        if num_scraped == 10: # exit for loop once we've collected 10 links on this page\n",
    "            break\n",
    "    \n",
    "    # assign last link collected as new url\n",
    "    url = url_info['link_url'][-1]\n",
    "\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        parsed_page = BeautifulSoup(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dict to dataframe\n",
    "url_df = pd.DataFrame(url_info)\n",
    "\n",
    "# add quotes around link_title entries\n",
    "url_df['link_title'] = '\"' + url_df['link_title'] + '\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv('webcrawler.csv', sep=',', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d57ac51cee0f429fd496c420bc0b417e0d972b4c9200b0fa8dda27213b63b64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CSE801a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
